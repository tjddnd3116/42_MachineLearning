{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c804fd",
   "metadata": {},
   "source": [
    "# 트리의 앙상블\n",
    "\n",
    "### 앙상블 학습이 무엇인지 이해하고 다양한 앙상블 학습 알고리즘을 실습을 통해 배웁니다.\n",
    "\n",
    "## 정형 데이터와 비정형 데이터\n",
    "\n",
    "우리가 다룬 생선의 데이터, 와인의 데이터는 scv파일에 가지런히 정리되어 있었다. 이런 형태의 데이터를 **정형 데이터**라고 부른다.\n",
    "\n",
    "어떤 구조로 되어 있다는 뜻이다. 이런 데이터는 scv나 데이터베이스, 엑셀에 저장하기 쉽다.\n",
    "\n",
    "이와 반대되는 데이터를 **비정형 데이터**라고 한다.\n",
    "\n",
    "비정형 데이터는 데이터베이스나 엑셀로 표현하기 어려운 것들이다. 예를들어 텍스트 데이터, 사진, 디지털 음악 등이 있다.\n",
    "\n",
    "정형 데이터를 다루는데 가장 뛰어난 성과를 내는 알고리즘이 **앙상블 학습**이다.\n",
    "\n",
    "이 알고리즘은 대부분 결정 트리를 기반으로 만들어져 있다.\n",
    "\n",
    "비정형 데이터에는 신경망 알고리즘을 사용한다.\n",
    "\n",
    "## 랜덤 포레스트\n",
    "\n",
    "**랜덤 포레스트**는 앙상블 학습의 대표주자로 안정적인 성능 덕분에 널리 사용되고 있다.\n",
    "\n",
    "랜덤 포레스트는 결정 트리를 랜덤하게 만들어 결정트리의 **숲**을 만든다.\n",
    "\n",
    "그리고 각 결정 트리의 예측을 사용해 최종 예측을 만든다.\n",
    "\n",
    "**부트스트랩 샘플**은 데이터 세트에서 중복을 허용하여 데이터를 샘플링 하는 방식이다. 그래서 훈련 세트와 크기가 같다.\n",
    "\n",
    "각 노드를 분할할 때 전체 특성 중에서 일부 특성을 무작위로 고른 다음 이 중에서 최선의 분할을 찾는다.\n",
    "\n",
    "분류 모델인 RandomForestClassifier는 기본적으로 전체 특성 개수의 제곱근만큼의 특성을 선택한다.\n",
    "\n",
    "회귀 모델인 RandomForestRegressor는 전체 특성을 사용한다.\n",
    "\n",
    "![사진](./forest.jpeg)\n",
    "\n",
    "랜덤 포레스트는 기본적으로 100개의 결정 트리를 이런 방식으로 훈련한다.\n",
    "\n",
    "분류일때는 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측으로 삼고\n",
    "\n",
    "회귀일때는 단순히 각 트리의 예측을 평균한다.\n",
    "\n",
    "랜덤 포레스트는 랜덤하게 선택한 샘플과 특성을 사용하기 때문에 훈련 세트에 과대적합되는 것을 막아주고 검증세트와 테스트세트에서 안정적인 성능을 얻을 수 있다.\n",
    "\n",
    "그럼 사이킷런의 RandomForestClassifier 클래스를 화이트 와인을 분류하는 문제에 적용해 보자.\n",
    "\n",
    "와인 데이터셋을 판다스로 불러오고 훈련 세트와 테스트 세트로 나눈다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a883cb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
    "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
    "target = wine['class'].to_numpy()\n",
    "train_input, test_input, train_target, test_target = train_test_split(\n",
    "data, target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a463cd6",
   "metadata": {},
   "source": [
    "cross_validate() 함수를 사용해 교차 검증을 수행해보자.\n",
    "\n",
    "return_train_score 매개변수를 True로 지정하면 검증 점수뿐만 아니라 훈련 세트에 대한 점수도 같이 반환된다.\n",
    "\n",
    "훈련 세트와 검증 세트의 점수를 비교하면 과대적합을 파악하는데 용이하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62ae2a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9973541965122431 0.8905151032797809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "scores = cross_validate(rf, train_input, train_target,\n",
    "                       return_train_score=True, n_jobs=-1)\n",
    "print(np.mean(scores['train_score'],), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ed463",
   "metadata": {},
   "source": [
    "출력된 결과를 보면 훈련 세트에 다소 과대적합된 것 같다.\n",
    "\n",
    "랜덤 포레스트는 결정 트리의 앙상블이기 때문에 결정 트리의 큰 장점중 하나인 특성중요도를 계산한다.\n",
    "\n",
    "랜덤 포레스트의 특성 중요도는 각 결정 트리의 특성 중요도를 취합한 것이다.\n",
    "\n",
    "랜덤 포레스트 모델을 훈련 세트에 훈련한 후 특성 중요도를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84b6c43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23167441 0.50039841 0.26792718]\n"
     ]
    }
   ],
   "source": [
    "rf.fit(train_input, train_target)\n",
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0515750c",
   "metadata": {},
   "source": [
    "앞의 1절 결정트리에서 만든 특성중요도와 비교해보자.\n",
    "\n",
    "[0.12345626 0.86862934 0.0079144]\n",
    "\n",
    "비교해 보면 당도의 중요도가 감소하고 알코올 도수와 pH 특성의 중요도가 조금 상승했다.\n",
    "\n",
    "이런 이유는 랜덤 포레스트가 특성의 일부를 랜덤하게 선택하여 결정트리를 훈련하기 때문이다.\n",
    "\n",
    "하나의 특성에 과도하게 집중하지 않고 좀 더 많은 특성이 훈련에 기여할 기회를 얻는다.\n",
    "\n",
    "이는 과대적합을 줄이고 일반화 성능을 높이는데 도움이 된다.\n",
    "\n",
    "RandomForestClassifier에는 재미있는 기능이 하나 더 있는데 부트스트랩 샘플에 포함되지 않고 남는 샘플을 **OOB 샘플**이라고 한다.\n",
    "\n",
    "이 남는 샘플을 사용하여 부트스트랩 샘플로 훈련한 결정 트리를 평가할 수 있다. 마치 **검정 세트**의 역할을 하는것이다.\n",
    "\n",
    "oob_score 매개변수를 True로 지정하면 랜덤 포레스트는 각 결정 트리의 OOB 점수를 평균하여 출력한다.\n",
    "\n",
    "한번 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b314ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8934000384837406\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
    "rf.fit(train_input, train_target)\n",
    "print(rf.oob_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07190df",
   "metadata": {},
   "source": [
    "교차 검증에서 얻은 점수와 매우 비슷한 결과를 얻었다.\n",
    "\n",
    "OOB점수를 사용하면 교차 검증을 대신할 수 있어서 결과적으로 훈련 세트에 더 많은 샘플을 사용할 수 있다.\n",
    "\n",
    "## 엑스트라 트리\n",
    "\n",
    "**엑스트라 트리**는 랜덤 포레스트와 매우 비슷하게 동작한다.\n",
    "\n",
    "전체 특성 중에 일부 특성을 랜덤하게 선택하여 노드를 분할하는데 사용한다.\n",
    "\n",
    "차이점은 부트스트랩 샘플을 사용하지 않는다는 점이다.\n",
    "\n",
    "결정 트리를 만들 때 전체 훈련 세트를 사용한다. 대신 노드를 분할할때 무작위로 분할한다.\n",
    "\n",
    "하나의 결정 트리에서 특성을 무작위로 분할 한다면 성능은 낮아지겠지만 과대적합을 막고 검증 세트의 점수를 높이는 효과가 있다.\n",
    "\n",
    "이 모델의 교차 검증 점수를 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92801bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9974503966084433 0.8887848893166506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
    "scores = cross_validate(et, train_input, train_target,\n",
    "                       return_train_score=True, n_jobs=-1)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61edd1a",
   "metadata": {},
   "source": [
    "랜덤 포레스트와 비슷한 결과를 얻었다. 이 예제는 특성이 많지 않아 두 모델의 차이가 크지 않다.\n",
    "\n",
    "엑스트라 트리가 무작위성이 좀 더 크기 때문에 랜덤 포레스트 보다 더 많은 결정트리를 훈련해야 한다.\n",
    "\n",
    "하지만 빠른 계산속도가 엑스트라 트리의 장점이다.\n",
    "\n",
    "엑스트라 트리도 특성 중요도를 제공한다.\n",
    "\n",
    "결과를 보면 엑스트라 트리도 결정트리보다 당도에 대한 의존성이 작다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dec13b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20183568 0.52242907 0.27573525]\n"
     ]
    }
   ],
   "source": [
    "et.fit(train_input, train_target)\n",
    "print(et.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994da2f9",
   "metadata": {},
   "source": [
    "## 그레이디언트 부스팅\n",
    "\n",
    "**그레이디언트 부스팅**은 깊이가 얕은 결정 트리를 사용하여 이전트리의 오차를 보완한다.\n",
    "\n",
    "기본적으로 깊이가 3인 결정트리를 100개 사용한다.\n",
    "\n",
    "그래서 과대적합에 강하고 일반적으로 높은 일반화 성능을 기대할 수 있다.\n",
    "\n",
    "그레이디언트 부스팅은 결정 트리를 계속 추가하면서 가장 낮은 곳으로 찾아 이동한다.\n",
    "\n",
    "손실함수의 낮은 곳으로 천천히 조금씩 이동해야 한다고 했는데 그래서 깊이가 얕은 트리를 사용한다.\n",
    "\n",
    "또 학습률 매개변수로 속도를 조절한다.\n",
    "\n",
    "GradientBoostingClassifier를 사용해 와인 데이터셋의 교차 검증 점수를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ebf6b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8881086892152563 0.8720430147331015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "scores = cross_validate(gb, train_input, train_target,\n",
    "                       return_train_score=True, n_jobs=-1)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75862622",
   "metadata": {},
   "source": [
    "그레이디언트 부스팅은 결정 트리의 개수를 늘려도 과대적합에 매우 강하다.\n",
    "\n",
    "학습률을 증가시키고 트리의 개수를 늘리면 조금 더 성능이 향상될 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eb3c302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9464595437171814 0.8780082549788999\n"
     ]
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2,\n",
    "                               random_state=42)\n",
    "scores = cross_validate(gb, train_input, train_target,\n",
    "                       return_train_score=True, n_jobs=-1)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edfc0e7",
   "metadata": {},
   "source": [
    "결정 트리 개수를 500개로 늘렸지만 과대적합을 잘 억제하고 있다.\n",
    "\n",
    "그레이디언트 부스팅도 특성 중요도를 제공한다.\n",
    "\n",
    "그레이디언트 부스팅이 랜덤 포레스트 보다 일부 특성(당도)에 더 집중한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad9bdf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15872278 0.68010884 0.16116839]\n"
     ]
    }
   ],
   "source": [
    "gb.fit(train_input, train_target)\n",
    "print(gb.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b47bb",
   "metadata": {},
   "source": [
    "재미있는 매개변수가 하나 있는데 트리 훈련에 사용할 훈련 세트의 비율을 정하는 subsample이다.\n",
    "\n",
    "이 매개변수의 기본값은 1로 전체 훈련 세트를 사용한다. 1보다 작으면 훈련세트의 일부를 사용한다. 마치 확률적 경사 하강법이나 미니배치 경사하강법과 비슷하다.\n",
    "\n",
    "그레이디언트 부스팅이 랜덤 포레스트보다 조금 더 높은 성능을 얻을 수있지만 순서대로 트리를 추가하기 때문에 훈련 속도가 느리다.\n",
    "\n",
    "## 히스토그램 기반 그레이디언트 부스팅\n",
    "\n",
    "**히스토그램 기반 그레이디언트 부스팅**은 정형 데이터를 다루는 머신러닝 알고리즘이다.\n",
    "\n",
    "히스토그램 기반 그레이디언트 부스팅은 입력 특성을 256개의 구간으로 나눈다. 따라서 최적의 분할을 매우 빠르게 찾을 수 있다.\n",
    "\n",
    "256개의 구간 중에서 하나를 떼어 놓고 누락된 값을 위해서 사용한다. 그래서 따로 전처리할 필요가 없다.\n",
    "\n",
    "기본 매개변수에서 안정적인 성능을 얻을수 있고 부스팅 반복 횟수를 지정하는 max_iter를 사용한다.\n",
    "\n",
    "그럼 와인 데이터셋에 HistGradientBoostingClassifier 클래스를 적용해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba444c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9321723946453317 0.8801241948619236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "hgb = HistGradientBoostingClassifier(random_state=42)\n",
    "scores = cross_validate(hgb, train_input, train_target,\n",
    "                       return_train_score=True)\n",
    "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de2abf",
   "metadata": {},
   "source": [
    "과대적합을 잘 억제 하면서 그레이디언트 부스팅보다 조금 더 높은 성능을 제공한다.\n",
    "\n",
    "특성 중요도를 계산하기위해 permutation_importance() 함수를 사용한다. \n",
    "\n",
    "이 함수는 특성을 하나씩 랜덤하게 섞어서 모델의 성능이 변화하는지를 관찰하여 어떤 특성이 중요한지를 계산한다.\n",
    "\n",
    "n_repeats 매개변수는 랜덤하게 섞을 횟수를 지정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e99d774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08876275 0.23438522 0.08027708]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "hgb.fit(train_input, train_target)\n",
    "result = permutation_importance(hgb, train_input, train_target,\n",
    "                               n_repeats=10, random_state=42, n_jobs=-1)\n",
    "print(result.importances_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f6ad5f",
   "metadata": {},
   "source": [
    "permutation_importance() 함수가 반환하는 객체는 특성중요도, 평균, 표준편차를 담고 있다.\n",
    "\n",
    "평균을 출력해보면 랜덤 포레스트와 비슷한 비율임을 알 수 있다.\n",
    "\n",
    "이번에는 테스트 세트에서 특성 중요도를 계산해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "97b297ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05969231 0.20238462 0.049     ]\n"
     ]
    }
   ],
   "source": [
    "result = permutation_importance(hgb, test_input, test_target, \n",
    "                               n_repeats=10, random_state=42, n_jobs=-1)\n",
    "print(result.importances_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb14835",
   "metadata": {},
   "source": [
    "테스트 세트의 결과를 보면 그레이디언트 부스팅과 비슷하게 조금 더 당도에 집중하고 있다는 것을 알 수 있다.\n",
    "\n",
    "이런 분석을 통해 모델을 실전에 투입했을 때 어떤 특성에 관심을 둘지 예상할 수 있다.\n",
    "\n",
    "그럼 HistGradientBoostingClassifier를 사용해 테스트 세트에서의 성능을 최종적으로 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a407579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8723076923076923"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hgb.score(test_input, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a48756",
   "metadata": {},
   "source": [
    "테스트 세트에서는 약 87%의 정확도를 얻었다.\n",
    "\n",
    "앙상블 모델은 확실히 단일 결정 트리보다 좋은 결과를 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70f7ae",
   "metadata": {},
   "source": [
    "## 앙상블 학습을 통한 성능 향상\n",
    "\n",
    "**앙상블 학습**은 강력하고 뛰어난 성능을 제공한다.\n",
    "\n",
    "앙상블 학습 알고리즘 중 랜덤포레스트, 엑스트라 트리, 그레이디언트 부스팅, 히스토그램 기반 그레이디언트 부스팅을 다루었다.\n",
    "\n",
    "**랜덤 포레스트**는 가장 대표적인 앙상블 학습 알고리즘이다. 성능이 좋고 안정적이다.\n",
    "\n",
    "결정트리를 훈련하기 위해 부트스트랩 샘플을 만들고 전체 특성 중 일부를 랜덤하게 선택하여 결정 트리를 만든다.\n",
    "\n",
    "**엑스트라 트리**는 부트스트랩을 사용하지 않고 노드를 분할할때 랜덤하게 분할한다. 훈련 속도가 빠르지만 보통 더 많은 트리가 필요하다.\n",
    "\n",
    "**그레이디언트 부스팅**은 깊이가 얕은 트리를 연속적으로 추가하여 손실 함수를 최소화하는 앙상블 방법이다.\n",
    "\n",
    "훈련속도가 조금 느리다, 학습률 매개변수를 조정하여 모델의 복잡도를 제어할 수 있다.\n",
    "\n",
    "학습률 매개변수가 크면 복잡하고 훈련 세트에 과대적합된 모델을 얻을 수 있다.\n",
    "\n",
    "**히스토그램 기반 그레이디언트 부스팅 알고리즘**은 훈련 데이터를 256개의 구간으로 변환하여 노드 분할 속도가 매우빠르다.\n",
    "\n",
    "지금 까지는 입력과 타깃이 준비된 문제를 풀었다. 이런 머신러닝 분야를 **지도 학습**이라고 한다.\n",
    "\n",
    "타깃이 없으면 어떨까?? 다음장에서 이에 대해 배워보자"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
